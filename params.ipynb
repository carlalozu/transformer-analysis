{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15f9be6",
   "metadata": {},
   "source": [
    "# Storage and compute analysis of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8af1d",
   "metadata": {},
   "source": [
    "List of models available https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d63c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bec15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_formatwarning(msg, *args, **kwargs):\n",
    "    # ignore everything except the message\n",
    "    return 'Warning: ' + str(msg) + '\\n'\n",
    "\n",
    "warnings.formatwarning = custom_formatwarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5f8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gigabytes(bytes: int) -> float:\n",
    "    return bytes / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00a5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "\n",
    "    n_layers:int\n",
    "    hidden_size:int # feed forward dimension\n",
    "    n_heads:int\n",
    "    n_microbatch:int=1 # for pipeline parallelism\n",
    "    s_length:int=512\n",
    "    vocab_size:int=50257\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.n_layers = config.n_layers\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.n_heads = config.n_heads\n",
    "        if hasattr(config, 'n_microbatch'):\n",
    "            self.n_microbatch = config.n_microbatch\n",
    "        else:\n",
    "            warnings.warn(\"Number of microbatches not found in config, defaulting to 1\")\n",
    "        if hasattr(config, 's_length'):\n",
    "            self.s_length = config.s_length\n",
    "        else:\n",
    "            warnings.warn(\"Sequence length not found in config, defaulting to 512\")\n",
    "        if hasattr(config, 'vocab_size'):\n",
    "            self.vocab_size = config.vocab_size\n",
    "        else:\n",
    "            warnings.warn(\"Vocabulary size not found in config, defaulting to 50257\")\n",
    "\n",
    "\n",
    "    def model_params(self) -> int:\n",
    "        \"\"\"In number of elements, LayerNorm params are included but may not be\n",
    "        necessary, since they are negligible compared to other parameters.\n",
    "        https://insujang.github.io/2022-07-30/analysis-of-transformer-model/#fn:3\n",
    "        In number of elements\n",
    "        \"\"\"\n",
    "\n",
    "        # Attention block parameters\n",
    "        params = self.n_layers * (\n",
    "            3 * self.hidden_size * self.hidden_size +  # Q, K, V\n",
    "            self.hidden_size * self.hidden_size +      # Output projection\n",
    "            2 * self.hidden_size                       # LayerNorm (gamma and beta)\n",
    "        )\n",
    "        # MLP parameters\n",
    "        params += self.n_layers * (\n",
    "            self.hidden_size * 4 * self.hidden_size +  # First linear layer\n",
    "            4 * self.hidden_size * self.hidden_size +  # Second linear layer\n",
    "            2 * self.hidden_size                       # LayerNorm (gamma and beta)\n",
    "        )\n",
    "        # Embedding parameters\n",
    "        params += self.vocab_size * self.hidden_size  # Token embeddings\n",
    "        params += self.s_length * self.hidden_size    # Position embeddings\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def gradients(self) -> int:\n",
    "        \"\"\"In number of elements\"\"\"\n",
    "        return self.model_params()\n",
    "    \n",
    "    def optimizer_states(self) -> int:\n",
    "        \"\"\"Adam stores a FP32 copy of parameters, plus momentum and variance estimates\n",
    "        In number of elements\"\"\"\n",
    "        return 3 * self.model_params()\n",
    "    \n",
    "    def activation_size(self) -> int:\n",
    "        \"\"\"Activation size calculation per layer in bytes, taken from NVIDIA's\n",
    "        analysis, https://arxiv.org/pdf/2205.05198\n",
    "        In bytes\"\"\"\n",
    "\n",
    "        # Attention activations\n",
    "        activations = (5*2 +1) * self.s_length * self.hidden_size \n",
    "        activations += 5 * self.n_heads * self.s_length * self.s_length\n",
    "        \n",
    "        # MLP activations\n",
    "        activations += (9*2 + 1) * self.s_length * self.hidden_size\n",
    "\n",
    "        # LayerNorm activations\n",
    "        activations += (2*2) * self.s_length * self.hidden_size\n",
    "\n",
    "        return activations * self.n_layers * self.n_microbatch\n",
    "    \n",
    "    def total_size(self) -> int:\n",
    "        \"\"\"Total size of model parameters, gradients, optimizer states, and activations\n",
    "        in bytes\"\"\"\n",
    "        total = 2 * self.model_params()  # parameters\n",
    "        total += 2 * self.gradients()    # gradients\n",
    "        total += 4 * self.optimizer_states()  # optimizer states\n",
    "        total += self.activation_size()   # activations\n",
    "        return total/1024/1024/1024  # in GB\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Properties\\n\"\n",
    "            f\"  Number of Layers: {self.n_layers}\\n\"\n",
    "            f\"  Hidden Size     : {self.hidden_size}\\n\"\n",
    "            f\"  Number of Heads : {self.n_heads}\\n\"\n",
    "            f\"  Microbatch Size : {self.n_microbatch}\\n\"\n",
    "            f\"  Sequence Length : {self.s_length}\\n\"\n",
    "            f\"  Vocabulary Size : {self.vocab_size}\\n\"\n",
    "        )\n",
    "\n",
    "    def print_parameters(self):\n",
    "        return (\n",
    "            f\"Total Parameters\\n\"\n",
    "            f\"  Model Parameters: {self.model_params()/1e6:.0f}M elements\\n\"\n",
    "            f\"                  : {gigabytes(2 * self.model_params()):.2f} GB\\n\"\n",
    "            f\"  Gradients       : {self.gradients()/1e6:.0f}M elements\\n\"\n",
    "            f\"                  : {gigabytes(2 * self.gradients()):.2f} GB\\n\"\n",
    "            f\"  Optimizer States: {self.optimizer_states()/1e6:.0f}M elements\\n\"\n",
    "            f\"                  : {gigabytes(4 * self.optimizer_states()):.2f} GB\\n\"\n",
    "            f\"  Activation Size : {gigabytes(self.activation_size()):.2f} GB\\n\"\n",
    "            f\"  Total Size      : {self.total_size():.2f} GB\\n\"\n",
    "        )\n",
    "    \n",
    "    def forward_flops_per_token(self) -> int:\n",
    "        \"\"\"FLOPs per token calculation for a forward pass\n",
    "        https://arxiv.org/pdf/2205.05198\n",
    "        \"\"\"\n",
    "\n",
    "        # Attention FLOPs\n",
    "        flops = self.n_layers * self.n_microbatch * (\n",
    "            # Q, K, V transformations\n",
    "            6 * self.hidden_size * self.hidden_size  * self.s_length +\n",
    "            # Attention matrix calculation\n",
    "            2 * self.hidden_size * self.s_length * self.s_length  +\n",
    "            # Attention over values\n",
    "            2 * self.hidden_size * self.s_length * self.s_length  +\n",
    "            # Post-attention linear projection\n",
    "            2 * self.s_length * self.s_length  * self.hidden_size \n",
    "        )\n",
    "\n",
    "        # MLP FLOPs (2 layers)\n",
    "        flops += self.n_layers * self.s_length * self.n_microbatch * (\n",
    "            2 * self.hidden_size * 4 * self.hidden_size +  # First linear layer\n",
    "            2 * 4 * self.hidden_size * self.hidden_size     # Second linear layer\n",
    "        )\n",
    "\n",
    "        # logits layer FLOPs\n",
    "        flops += 2 * self.n_microbatch * self.s_length * self.hidden_size * self.vocab_size\n",
    "\n",
    "        return flops\n",
    "    \n",
    "    def backward_flops_per_token(self) -> int:\n",
    "        # Backward pass FLOPs is roughly twice the forward pass FLOPs\n",
    "        # if using activation recomputation, it's 3x\n",
    "        return 2 * self.forward_flops_per_token()\n",
    "    \n",
    "    def print_flops(self):\n",
    "        fwd_flops = self.forward_flops_per_token()\n",
    "        bwd_flops = self.backward_flops_per_token()\n",
    "        total_flops = fwd_flops + bwd_flops\n",
    "\n",
    "        return (\n",
    "            f\"FLOPs per Token\\n\"\n",
    "            f\"  Forward Pass : {fwd_flops/1e9:.0f} GFLOPs\\n\"\n",
    "            f\"  Backward Pass: {bwd_flops/1e9:.0f} GFLOPs\\n\"\n",
    "            f\"  Total        : {total_flops/1e9:.0f} GFLOPs\\n\"\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0e23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 62\n",
      "  Hidden Size     : 7168\n",
      "  Number of Heads : 12\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 512\n",
      "  Vocabulary Size : 129280\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 39159M elements\n",
      "                  : 72.94 GB\n",
      "  Gradients       : 39159M elements\n",
      "                  : 72.94 GB\n",
      "  Optimizer States: 117477M elements\n",
      "                  : 437.64 GB\n",
      "  Activation Size : 8.11 GB\n",
      "  Total Size      : 591.63 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 37530 GFLOPs\n",
      "  Backward Pass: 75060 GFLOPs\n",
      "  Total        : 112591 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DeepSeekV32Exp:\n",
    "    # tried decoding from https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp#deepseek-v32-exp\n",
    "    n_layers:int=62\n",
    "    hidden_size:int=7168 # feed forward dimension\n",
    "    n_heads:int=12 # guessing, seems low\n",
    "    n_microbatch:int=1 # for pipeline parallelism\n",
    "    s_length:int=512\n",
    "    vocab_size:int=129280\n",
    "    \n",
    "deepseekv32 = Params(DeepSeekV32Exp())\n",
    "print(deepseekv32)\n",
    "print(deepseekv32.print_parameters())\n",
    "print(deepseekv32.print_flops())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362e74bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 12\n",
      "  Hidden Size     : 768\n",
      "  Number of Heads : 12\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 512\n",
      "  Vocabulary Size : 50257\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 124M elements\n",
      "                  : 0.23 GB\n",
      "  Gradients       : 124M elements\n",
      "                  : 0.23 GB\n",
      "  Optimizer States: 372M elements\n",
      "                  : 1.39 GB\n",
      "  Activation Size : 0.33 GB\n",
      "  Total Size      : 2.17 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 134 GFLOPs\n",
      "  Backward Pass: 267 GFLOPs\n",
      "  Total        : 401 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Number of microbatches not found in config, defaulting to 1\n",
      "Warning: Sequence length not found in config, defaulting to 512\n",
      "Warning: Vocabulary size not found in config, defaulting to 50257\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BERTConfig:\n",
    "    # Bert Based Uncased, random old example I found in Huggingface\n",
    "    n_layers: int = 12\n",
    "    hidden_size: int = 768\n",
    "    n_heads: int = 12\n",
    "\n",
    "bert = Params(BERTConfig())\n",
    "print(bert)\n",
    "print(bert.print_parameters())\n",
    "print(bert.print_flops())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d90f8919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 12\n",
      "  Hidden Size     : 768\n",
      "  Number of Heads : 12\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 2048\n",
      "  Vocabulary Size : 50257\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 125M elements\n",
      "                  : 0.23 GB\n",
      "  Gradients       : 125M elements\n",
      "                  : 0.23 GB\n",
      "  Optimizer States: 375M elements\n",
      "                  : 1.40 GB\n",
      "  Activation Size : 3.41 GB\n",
      "  Total Size      : 5.27 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 709 GFLOPs\n",
      "  Backward Pass: 1418 GFLOPs\n",
      "  Total        : 2127 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPT3Small:\n",
    "    # https://en.wikipedia.org/wiki/GPT-3#GPT-3_models\n",
    "    # https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters\n",
    "    n_layers:int=12\n",
    "    hidden_size:int=768 # d_model\n",
    "    n_heads:int=12\n",
    "    n_microbatch:int=1 # for pipeline parallelism\n",
    "    s_length:int=2048\n",
    "    vocab_size:int=50257\n",
    "    \n",
    "gpt3small = Params(GPT3Small())\n",
    "print(gpt3small)\n",
    "print(gpt3small.print_parameters())\n",
    "print(gpt3small.print_flops())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bfff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 24\n",
      "  Hidden Size     : 1024\n",
      "  Number of Heads : 16\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 2048\n",
      "  Vocabulary Size : 50257\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 356M elements\n",
      "                  : 0.66 GB\n",
      "  Gradients       : 356M elements\n",
      "                  : 0.66 GB\n",
      "  Optimizer States: 1067M elements\n",
      "                  : 3.97 GB\n",
      "  Activation Size : 9.09 GB\n",
      "  Total Size      : 14.39 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 1963 GFLOPs\n",
      "  Backward Pass: 3926 GFLOPs\n",
      "  Total        : 5889 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPT3Medium:\n",
    "    # https://en.wikipedia.org/wiki/GPT-3#GPT-3_models\n",
    "    # https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters\n",
    "    n_layers:int=24\n",
    "    hidden_size:int=1024 # d_model\n",
    "    n_heads:int=16\n",
    "    s_length:int=2048\n",
    "    vocab_size:int=50257\n",
    "    n_microbatch:int=1\n",
    "\n",
    "gpt3medium = Params(GPT3Medium())\n",
    "print(gpt3medium)\n",
    "print(gpt3medium.print_parameters())\n",
    "print(gpt3medium.print_flops())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e948649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 24\n",
      "  Hidden Size     : 2048\n",
      "  Number of Heads : 24\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 2048\n",
      "  Vocabulary Size : 50257\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 1315M elements\n",
      "                  : 2.45 GB\n",
      "  Gradients       : 1315M elements\n",
      "                  : 2.45 GB\n",
      "  Optimizer States: 3946M elements\n",
      "                  : 14.70 GB\n",
      "  Activation Size : 14.44 GB\n",
      "  Total Size      : 34.04 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 6194 GFLOPs\n",
      "  Backward Pass: 12388 GFLOPs\n",
      "  Total        : 18582 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPT3XL:\n",
    "    # https://en.wikipedia.org/wiki/GPT-3#GPT-3_models\n",
    "    # https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters\n",
    "    n_layers:int=24\n",
    "    hidden_size:int=2048 # d_model\n",
    "    n_heads:int=24\n",
    "    s_length:int=2048\n",
    "    vocab_size:int=50257\n",
    "    n_microbatch:int=1\n",
    "\n",
    "gpt3xl = Params(GPT3XL())\n",
    "print(gpt3xl)\n",
    "print(gpt3xl.print_parameters())\n",
    "print(gpt3xl.print_flops())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ddecc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties\n",
      "  Number of Layers: 96\n",
      "  Hidden Size     : 12288\n",
      "  Number of Heads : 96\n",
      "  Microbatch Size : 1\n",
      "  Sequence Length : 2048\n",
      "  Vocabulary Size : 50257\n",
      "\n",
      "Total Parameters\n",
      "  Model Parameters: 174594M elements\n",
      "                  : 325.21 GB\n",
      "  Gradients       : 174594M elements\n",
      "                  : 325.21 GB\n",
      "  Optimizer States: 523781M elements\n",
      "                  : 1951.24 GB\n",
      "  Activation Size : 256.50 GB\n",
      "  Total Size      : 2858.15 GB\n",
      "\n",
      "FLOPs per Token\n",
      "  Forward Pass : 685326 GFLOPs\n",
      "  Backward Pass: 1370652 GFLOPs\n",
      "  Total        : 2055979 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPT3175B:\n",
    "    # https://en.wikipedia.org/wiki/GPT-3#GPT-3_models\n",
    "    # https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters\n",
    "    n_layers:int=96\n",
    "    hidden_size:int=12288 # d_model\n",
    "    n_heads:int=96\n",
    "    s_length:int=2048\n",
    "    vocab_size:int=50257\n",
    "    n_microbatch:int=1\n",
    "\n",
    "gpt3175B = Params(GPT3175B())\n",
    "print(gpt3175B)\n",
    "print(gpt3175B.print_parameters())\n",
    "print(gpt3175B.print_flops())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610f0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
